{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_TFHub_Deploy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishantparmar24/Political-Risk/blob/master/BERT_TFHub_Deploy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ",
        "colab_type": "text"
      },
      "source": [
        "#Predicting Sentiment with BERT on TF Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import regularizers\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5wfXDx5SPH",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-tensorflow boto3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "from tensorflow import keras\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uceT-zk88PGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkSU4BA6Pxf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR = '/gdrive/My Drive/Political Risk Project/'\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'BertAnalysis', 'ModelOutput5')\n",
        "# OUTPUT_DIR = 'bert_model'\n",
        "ACTUAL_PREDS = os.path.join(BASE_DIR, \n",
        "                            'Test Data',\n",
        "                            'PredictionAnalysis', \n",
        "                            'reviewed_predictions.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom_ff20gyy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "imdb_train, imdb_test = download_and_load_datasets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTMPUH4WMgxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdb_train = imdb_train[['sentence', 'polarity']]\n",
        "imdb_test = imdb_test[['sentence', 'polarity']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsW1DoA-i2sJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "max_length = 16\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "# Train on at least 160000 to see the best effects\n",
        "training_size = 160000\n",
        "test_portion=.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwqpcFeTi5XG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "# Cleaning the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader\n",
        "# command: iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n",
        "    -O /tmp/training_cleaned.csv\n",
        "\n",
        "num_sentences = 0\n",
        "\n",
        "with open(\"/tmp/training_cleaned.csv\") as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "        list_item=[]\n",
        "        label_ = 0 if row[0] == '0' else 1\n",
        "        list_item.extend([row[5], label_])\n",
        "        num_sentences = num_sentences + 1\n",
        "        corpus.append(list_item)\n",
        "\n",
        "sentences=[]\n",
        "labels=[]\n",
        "random.shuffle(corpus)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abfwdn-g135",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train, test = download_and_load_datasets()\n",
        "data_ = list()\n",
        "for x in range(training_size):\n",
        "    row_ = dict()\n",
        "    row_['sentence'] = corpus[x][0]\n",
        "    row_['polarity'] = corpus[x][1]\n",
        "    data_.append(row_)\n",
        "full_df = pd.DataFrame(data_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdwgtCKlwrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split = int(test_portion * training_size)\n",
        "tw_test = full_df[:split]\n",
        "tw_train = full_df[split:training_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uI02SqutBPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual_scores = pd.read_csv(ACTUAL_PREDS)[[\"headline_text\", \"actual_score\"]]\n",
        "actual_scores.rename(columns={\"headline_text\": \"sentence\", \n",
        "                              \"actual_score\": \"polarity\"},\n",
        "                     inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUhcvNwr4RmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual_scores_neg = actual_scores[actual_scores[\"polarity\"] == 0]\n",
        "neg_scores = actual_scores_neg.shape[0]\n",
        "actual_scores_pos = actual_scores[actual_scores[\"polarity\"] == 1]\n",
        "pos_scores = actual_scores_pos.shape[0]\n",
        "actual_scores_new = actual_scores_neg[neg_scores-pos_scores:]\n",
        "actual_scores_new = actual_scores_new.append(actual_scores_pos)\n",
        "actual_scores_new = actual_scores_new.sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oAH9vcfwyW8O",
        "colab": {}
      },
      "source": [
        "actual_score_split = int(0.8 * actual_scores_new.shape[0])\n",
        "train_space = actual_scores_new[:actual_score_split].sample(frac=1)\n",
        "test_space = actual_scores_new[actual_score_split:].sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irTXuKJs5fp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_space = train_space.append(imdb_train.sample(10000))\n",
        "test_space = test_space.append(imdb_test.sample(10000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_F488eixTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sample = train_space.append(tw_train.sample(10000), sort=False)\n",
        "test_sample = test_space.append(tw_test.sample(10000), sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRnHSz3iSXz",
        "colab_type": "text"
      },
      "source": [
        "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'polarity'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train_sample.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test_sample.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZWZtKxObjh",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU",
        "colab_type": "text"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oFkhpZBDKm",
        "colab_type": "text"
      },
      "source": [
        "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBo6RCtQmwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc",
        "colab_type": "text"
      },
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE",
        "colab_type": "text"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where the learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOO3RfG1DYLo",
        "colab_type": "text"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-",
        "colab_type": "text"
      },
      "source": [
        "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3",
        "colab_type": "text"
      },
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKsULteiz1B",
        "colab_type": "text"
      },
      "source": [
        "Now let's write code to make predictions on new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsrbTD2EJTVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPrediction(in_sentences):\n",
        "  labels = [\"Negative\", \"Positive\"]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8AUvedN7kTYk",
        "colab": {}
      },
      "source": [
        "def clean_file_text(file_text):\n",
        "    ptrn_ = \"(\\w+\\.\\s\\d+)\"\n",
        "    text_ = re.subn(\"\\n\", \" \", file_text)[0]\n",
        "    text_ = re.subn(\"(\\.\\s\\()\", \".(\", text_)[0]\n",
        "    match_num = re.match(ptrn_, text_) or re.search(ptrn_, text_)\n",
        "    if match_num:\n",
        "        m_split = re.split(ptrn_, text_)\n",
        "        for item in m_split:\n",
        "            m = re.match(\"(\\.\\s)(\\d+)\", item) or re.search(\"(\\.\\s)(\\d+)\", item)\n",
        "            if m:\n",
        "                repl_ = \"\".join(item.split(\" \"))\n",
        "                text_ = text_.replace(item, repl_)\n",
        "    return text_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H3rv_mIGkSUS",
        "colab": {}
      },
      "source": [
        "repo = '/gdrive/My Drive/Political Risk Project/Test Data/PredictionAnalysis'\n",
        "articles_repo = '/gdrive/My Drive/Political Risk Project/Test Data/'\n",
        "class_0 = os.path.join(articles_repo, 'Snap Election', '0')\n",
        "class_1 = os.path.join(articles_repo, 'Snap Election', '1')\n",
        "articles_0, articles_1 = os.listdir(class_0), os.listdir(class_1)\n",
        "class_0_files = [f for f in map(lambda x: os.path.join(class_0, x), \n",
        "                                  articles_0)]\n",
        "class_1_files = [f for f in map(lambda x: os.path.join(class_1, x), \n",
        "                                  articles_1)]\n",
        "# all_class_files\n",
        "file_labels = {\"no snap\": class_0_files,\n",
        "               \"snap\": class_1_files}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FLnjkXtEkRpa",
        "colab": {}
      },
      "source": [
        "def get_sentence_level_prediction(class_):\n",
        "    occurrences = dict()\n",
        "    for filename in class_:\n",
        "        # fpath = os.path.join(class_, filename)\n",
        "        # print(\"\\nFile path: {}\".format(fpath))\n",
        "        with open(filename, encoding='cp1252') as f:\n",
        "            file_contents = f.read()\n",
        "        file_clean = re.subn(r'[“”,]', '', clean_file_text(file_contents))\n",
        "        file_sentences = re.split('\\.\\s+', file_clean[0].strip())\n",
        "        article_prediction = pd.DataFrame(\n",
        "            getPrediction(pd.Series(file_sentences)))\n",
        "        article_prediction.rename(\n",
        "            columns={0: 'sentence', 1: 'logits', 2: 'label'},\n",
        "            inplace=True)\n",
        "        occurrences[filename] = article_prediction\n",
        "    return occurrences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tyug3srrMeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyze_file_text(file_dict):\n",
        "    message = \"\"\n",
        "    file_text_collection = pd.DataFrame()\n",
        "    sentence_collection = pd.DataFrame()\n",
        "    for label, files in file_dict.items():\n",
        "        message += \"\\nPredictions for file classes: {}\".format(label)\n",
        "        occurrences = get_sentence_level_prediction(files)\n",
        "        collection_ = list()\n",
        "        sentence_collection_ = list()\n",
        "        for fn, target in occurrences.items():\n",
        "            file_id = \"{}_{}\".format(\"_\".join(label.split(\" \")),\n",
        "                                    fn.split(os.path.sep)[-1])\n",
        "            file_stats = dict()\n",
        "            sentence_predictions = dict()\n",
        "            message+=\"file: {}\".format(file_id)\n",
        "            file_stats['file'] = file_id\n",
        "            file_stats['neg_count'] = target[target['label'] == \"Negative\"].shape[0]\n",
        "            file_stats['pos_count'] = target[target['label'] == \"Positive\"].shape[0]\n",
        "            sentence_predictions['file'] = file_id\n",
        "            sentence_predictions['negative_predictions'] = list(\n",
        "                target[target['label'] == \"Negative\"][\"sentence\"])\n",
        "            sentence_predictions['positive_predictions'] = list(\n",
        "                target[target['label'] == \"Positive\"][\"sentence\"])\n",
        "            message += \"\\n{} Negative:Positive = {}:{}\".format(fn,\n",
        "                target[target['label'] == \"Negative\"].shape[0], \n",
        "                target[target['label'] == \"Positive\"].shape[0])\n",
        "            collection_.append(file_stats)\n",
        "            sentence_collection_.append(sentence_predictions)\n",
        "\n",
        "        collection_df = pd.DataFrame(collection_)\n",
        "        if file_text_collection.empty:\n",
        "            file_text_collection = collection_df\n",
        "        else:\n",
        "            file_text_collection = file_text_collection.append(collection_df)\n",
        "        \n",
        "        collection_df = pd.DataFrame(sentence_collection_)\n",
        "        if sentence_collection.empty:\n",
        "            sentence_collection = collection_df\n",
        "        else:\n",
        "            sentence_collection = sentence_collection.append(collection_df)\n",
        "    return (file_text_collection, sentence_collection, message)\n",
        "        # file_text_collection.append(collection_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU-WaT0QCUXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To write the Neg/Pos Sentence predictions\n",
        "def write_sentence_level_pred(sentence_collection):\n",
        "    sentence_collection.reset_index(inplace=True)\n",
        "    sentence_collection.drop(columns=['index'], inplace=True)\n",
        "    files = list(sentence_collection['file'])\n",
        "    for file_ in files:\n",
        "        n = sentence_collection[sentence_collection['file']==file_].squeeze()[\"negative_predictions\"]\n",
        "        p = sentence_collection[sentence_collection['file']==file_].squeeze()[\"positive_predictions\"]\n",
        "        n = \"\\n\\n\".join(n)\n",
        "        p = \"\\n\\n\".join(p)\n",
        "\n",
        "        with open(\n",
        "            os.path.join(\n",
        "                repo, 'ReutersArticles', 'Negative', file_), 'w') as f:\n",
        "            f.write(n)\n",
        "        with open(\n",
        "            os.path.join(\n",
        "                repo, 'ReutersArticles', 'Positive', file_), 'w') as f:\n",
        "            f.write(p)\n",
        "# write_sentence_level_pred(sentence_collection)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PI7o0MTkMJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def visualize_sentiment_stats(stats_df, col_list):\n",
        "    likert_colors = ['white', 'firebrick', 'darkblue']\n",
        "    dummy = stats_df.copy()\n",
        "    middles = dummy[col_list[:-1]].sum(axis=1) + dummy[col_list[-1]]\n",
        "    longest = middles.max()\n",
        "    complete_longest = dummy.sum(axis=1).max()\n",
        "    dummy.insert(0, '', (middles - longest).abs())\n",
        "\n",
        "    dummy.plot.barh(stacked=True, color=likert_colors, edgecolor='none', \n",
        "                    legend=False)\n",
        "    z = plt.axvline(longest, linestyle='--', color='black', alpha=.5)\n",
        "    z.set_zorder(-1)\n",
        "\n",
        "    plt.xlim(0, complete_longest)\n",
        "    xvalues = range(0,complete_longest,10)\n",
        "    xlabels = [str(x-longest) for x in xvalues]\n",
        "    plt.xticks(xvalues, xlabels)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# visualize_sentiment_stats(file_text_stats, file_text_stats.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYWWJiZp5AbD",
        "colab_type": "text"
      },
      "source": [
        "## Deploy model to S3:\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3qLvoSZ5BVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def serving_input_fn():\n",
        "    reciever_tensors = {\n",
        "        \"input_ids\": tf.placeholder(dtype=tf.int32,\n",
        "                                    shape=[1, MAX_SEQ_LENGTH])\n",
        "    }\n",
        "    features = {\n",
        "        \"input_ids\": reciever_tensors['input_ids'],\n",
        "        \"input_mask\": 1 - tf.cast(tf.equal(reciever_tensors['input_ids'], 0), dtype=tf.int32),\n",
        "        \"segment_ids\": tf.zeros(dtype=tf.int32, shape=[1, MAX_SEQ_LENGTH]),\n",
        "        \"label_ids\": tf.placeholder(tf.int32, [None], name='label_ids')\n",
        "    }\n",
        "    return tf.estimator.export.ServingInputReceiver(features, reciever_tensors)\n",
        "    \n",
        "estimator._export_to_tpu = False\n",
        "estimator.export_saved_model(OUTPUT_DIR+\"/export\", serving_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq9g2gXt5Zd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aws_credentials = '/gdrive/My Drive/AWS/AP/credentials'\n",
        "\n",
        "\n",
        "def get_credentials(param='default'):\n",
        "    with open(aws_credentials) as f:\n",
        "        creds = f.read()\n",
        "    creds_split = creds.split('\\n')\n",
        "    creds_dict = dict()\n",
        "    prev_key = None\n",
        "    for line in creds_split:\n",
        "        if not re.search('=', line):\n",
        "            match = re.search('\\[(.*)\\]', line)\n",
        "            if match:\n",
        "                key = match.groups()[0]\n",
        "                creds_dict[key] = dict()\n",
        "                prev_key = key\n",
        "        else:\n",
        "            if prev_key:\n",
        "                creds_dict[prev_key].update(\n",
        "                    {line.split('=')[0]:line.split('=')[1]})\n",
        "    if param in creds_dict.keys():\n",
        "        return creds_dict[param]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnFob57LkK1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import boto3\n",
        "\n",
        "creds_ = get_credentials()\n",
        "S3_UPLOAD_PATH = \"s3://eiu-political-risk-nlp-model/bert\"\n",
        "try:\n",
        "    bucket = re.search(\"s3://(.+?)/\", S3_UPLOAD_PATH).group(1)\n",
        "    key = re.search(\"s3://.+?/(.+)\", S3_UPLOAD_PATH).group(1)\n",
        "except:\n",
        "    print(\"\\033[91m{}\\033[00m\".format(\n",
        "        \"ERROR: Invalid s3 path (should be of the form s3://my-bucket/path/to/file)\"), \n",
        "        file=sys.stderr)\n",
        "s3 = boto3.client(\"s3\", \n",
        "                  aws_access_key_id=creds_['aws_access_key_id'], \n",
        "                  aws_secret_access_key=creds_['aws_secret_access_key'])\n",
        "for dirpath, _, filenames in os.walk(OUTPUT_DIR+\"/export\"):\n",
        "    for filename in filenames:\n",
        "        filepath = os.path.join(dirpath, filename)\n",
        "        filekey = os.path.join(key, filepath[len(OUTPUT_DIR+\"/export/\"):])\n",
        "        print(\"Uploading s3://{}/{} ...\".format(bucket, filekey), end = '')\n",
        "        s3.upload_file(filepath, bucket, filekey)\n",
        "        print(\" ✓\")\n",
        "\n",
        "print(\"\\nUploaded model export directory to \" + S3_UPLOAD_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}